{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ea48bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.utils.data as Data\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from torch import optim\n",
    "\n",
    "# Update the import path to match your project structure\n",
    "from parse.parse_arg import parse_basic_args    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3130199",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"D:\\capstone_project\\capstone_project\\converted_pickle_file2_finall.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28a2517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "inner_edge = np.array(np.load(\"D:\\\\capstone_project\\\\capstone_project\\\\inner.npy\"))\n",
    "outer_edge = np.array(np.load(\"D:\\\\capstone_project\\\\capstone_project\\\\outer.npy\"))\n",
    "time_step = data[\"train\"][\"x1\"].shape[-2]\n",
    "input_dim = data[\"train\"][\"x1\"].shape[-1]\n",
    "num_weeks = data[\"train\"][\"x1\"].shape[0]\n",
    "train_size = int(num_weeks * 0.2)\n",
    "device = \"cpu\"\n",
    "agg_week_num = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe4caa91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83, 50, 7, 23)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']['x4'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3408dd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into torch dtype\n",
    "train_w1 = torch.Tensor(data[\"train\"][\"x1\"].astype(float)).float().to(device)\n",
    "train_w2 = torch.Tensor(data[\"train\"][\"x2\"].astype(float)).float().to(device)\n",
    "train_w3 = torch.Tensor(data[\"train\"][\"x3\"].astype(float)).float().to(device)\n",
    "train_w4 = torch.Tensor(data[\"train\"][\"x4\"].astype(float)).float().to(device)\n",
    "inner_edge = torch.tensor(inner_edge.T, dtype=torch.int64).to(device)\n",
    "outer_edge = torch.tensor(outer_edge.T, dtype=torch.int64).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7937c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "test_w1 = torch.Tensor(data[\"test\"][\"x1\"].astype(float)).float().to(device)\n",
    "test_w2 = torch.Tensor(data[\"test\"][\"x2\"].astype(float)).float().to(device)\n",
    "test_w3 = torch.Tensor(data[\"test\"][\"x3\"].astype(float)).float().to(device)\n",
    "test_w4 = torch.Tensor(data[\"test\"][\"x4\"].astype(float)).float().to(device)\n",
    "test_data = [test_w1, test_w2, test_w3, test_w4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2231687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label data\n",
    "train_reg = torch.Tensor(data[\"train\"][\"y_return_ratio\"].astype(float)).float()\n",
    "train_cls = torch.Tensor(data[\"train\"][\"y_up_or_down\"].astype(float)).float()\n",
    "test_y = data[\"test\"][\"y_return_ratio\"]\n",
    "test_cls = data[\"test\"][\"y_up_or_down\"]\n",
    "test_shape = test_y.shape[0]\n",
    "loop_number = 100\n",
    "ks_list = [5, 10, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85889d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(361, 50)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d7c8253",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self,time_step,dim):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.attention_matrix = nn.Linear(time_step, time_step)\n",
    "\n",
    "    def forward(self, inputs):  \n",
    "        inputs_t = torch.transpose(inputs,2,1) # (B, T, C) -> (B, C, T)\n",
    "        attention_weight = self.attention_matrix(inputs_t) # (B, C ,T) * (B, T ,T) -> (B, C, T)\n",
    "        attention_probs = F.softmax(attention_weight,dim=-1) # softmax along the last dim which means every row of C will have sum as 1 \n",
    "        attention_probs = torch.transpose(attention_probs,2,1) # (B, C, T) -> (B, T, C) now every col has sum = 1\n",
    "        attention_vec = torch.mul(attention_probs, inputs) # multiplying the softmax matrix with input\n",
    "        attention_vec = torch.sum(attention_vec,dim=1) # sum along the second dimension \n",
    "        return attention_vec, attention_probs\n",
    "\n",
    "class SequenceEncoder(nn.Module):\n",
    "    def __init__(self,input_dim,time_step,hidden_dim):\n",
    "        super(SequenceEncoder, self).__init__()\n",
    "        self.encoder = nn.GRU(input_size=input_dim,hidden_size=hidden_dim,num_layers=1,batch_first=True)\n",
    "        self.attention_block = AttentionBlock(time_step,hidden_dim) \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.dim = hidden_dim\n",
    "    \n",
    "    def forward(self,seq):\n",
    "        '''\n",
    "        inp : torch.tensor (batch,time_step,input_dim)\n",
    "        '''\n",
    "        seq_vector,_ = self.encoder(seq)\n",
    "        seq_vector = self.dropout(seq_vector)\n",
    "        attention_vec, _ = self.attention_block(seq_vector)\n",
    "        attention_vec = attention_vec.view(-1,1,self.dim) # prepare for concat\n",
    "        return attention_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7574451",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalGraphAtt(nn.Module):\n",
    "    def __init__(self,input_dim,time_step,hidden_dim,inner_edge,outer_edge,no_of_weeks_to_look_back,use_gru,device):\n",
    "        super(CategoricalGraphAtt, self).__init__()\n",
    "\n",
    "        # basic parameters\n",
    "        self.dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.time_step = time_step\n",
    "        self.inner_edge = inner_edge\n",
    "        self.outer_edge = outer_edge\n",
    "        self.no_of_weeks_to_look_back = no_of_weeks_to_look_back\n",
    "        self.use_gru = use_gru\n",
    "        self.device = device\n",
    "\n",
    "        # hidden layers\n",
    "        self.pool_attention = AttentionBlock(10,hidden_dim)\n",
    "        if self.use_gru:\n",
    "            self.weekly_encoder = nn.GRU(hidden_dim,hidden_dim)\n",
    "        self.encoder_list = nn.ModuleList([SequenceEncoder(input_dim,time_step,hidden_dim) for _ in range(no_of_weeks_to_look_back)]) \n",
    "        self.cat_gat = GATConv(hidden_dim,hidden_dim)\n",
    "        self.inner_gat = GATConv(hidden_dim,hidden_dim)\n",
    "        self.weekly_attention = AttentionBlock(no_of_weeks_to_look_back,hidden_dim)\n",
    "        self.fusion = nn.Linear(hidden_dim*3,hidden_dim)\n",
    "\n",
    "        # output layer \n",
    "        self.reg_layer = nn.Linear(hidden_dim,1)\n",
    "        self.cls_layer = nn.Linear(hidden_dim,1)\n",
    "\n",
    "    def forward(self,weekly_batch):\n",
    "        # print(\"0\")\n",
    "        # x has shape (category_num, stocks_num, time_step, dim)\n",
    "        # Taiwan has (5 sectors, and 100 stocks, T, C)  i think the time step is 1 all throughout\n",
    "        weekly_embedding = self.encoder_list[0](weekly_batch[0].view(-1,self.time_step,self.input_dim)) # (100,1,dim)\n",
    "        # print(\"1\")\n",
    "        # calculate embeddings for the rest of weeks\n",
    "        for week_idx in range(1,self.no_of_weeks_to_look_back):\n",
    "            # print(\"2.1\")\n",
    "            weekly_inp = weekly_batch[week_idx] # (category_num, stocks_num, time_step, dim)\n",
    "            # print(\"2.2\")\n",
    "            weekly_inp = weekly_inp.view(-1,self.time_step,self.input_dim) # reshape for faster training \n",
    "            # print(\"2.3\")\n",
    "            week_stock_embedding = self.encoder_list[week_idx](weekly_inp) # (100,1,dim)\n",
    "            # print(\"2.4\")\n",
    "            weekly_embedding = torch.cat((weekly_embedding,week_stock_embedding),dim=1)\n",
    "            # print(\"2.5\")\n",
    "        # print(\"3\")\n",
    "        # merge weeks \n",
    "        if self.use_gru:\n",
    "            weekly_embedding,_ = self.weekly_encoder(weekly_embedding)\n",
    "        weekly_att_vector,_ = self.weekly_attention(weekly_embedding) # (100,dim)\n",
    "        # print(\"4\")\n",
    "\n",
    "        # inner graph interaction \n",
    "        inner_graph_embedding = self.inner_gat(weekly_att_vector,self.inner_edge)\n",
    "        # print(\"5\")\n",
    "        inner_graph_embedding = inner_graph_embedding.view(5,10,-1)\n",
    "        # print(\"6\")\n",
    "\n",
    "        # pooling \n",
    "        weekly_att_vector = weekly_att_vector.view(5,10,-1)\n",
    "        # print(\"7\")\n",
    "        category_vectors,_ =  self.pool_attention(weekly_att_vector) #torch.max(weekly_att_vector,dim=1)\n",
    "        # print(\"8\")\n",
    "\n",
    "        # use category graph attention \n",
    "        category_vectors = self.cat_gat(category_vectors,self.outer_edge) # (5,dim)\n",
    "        # print(\"9\")\n",
    "        category_vectors = category_vectors.unsqueeze(1).expand(-1,10,-1)\n",
    "        #print(\"10\")\n",
    "\n",
    "        # fusion \n",
    "        fusion_vec = torch.cat((weekly_att_vector,category_vectors,inner_graph_embedding),dim=-1)  # I think its the Ti(A), Ti(G), Ti(Ï€c)\n",
    "        #print(\"11\")\n",
    "        fusion_vec = torch.relu(self.fusion(fusion_vec))\n",
    "        #print(\"12\")\n",
    "\n",
    "        # output\n",
    "        reg_output = self.reg_layer(fusion_vec)\n",
    "        #print(\"13\")\n",
    "        reg_output = torch.flatten(reg_output)\n",
    "        #print(\"14\")\n",
    "        cls_output = torch.sigmoid(self.cls_layer(fusion_vec))\n",
    "        # print(\"15\")\n",
    "        cls_output = torch.flatten(cls_output)\n",
    "        # print(\"16\")\n",
    "\n",
    "        return reg_output, cls_output\n",
    "\n",
    "    def predict_toprank(self,test_data,device,top_k=5):\n",
    "        # print(\"17\")\n",
    "        y_pred_all_reg, y_pred_all_cls = [], []\n",
    "        test_w1,test_w2,test_w3,test_w4 = test_data\n",
    "        for idx,_ in enumerate(test_w2):\n",
    "          if(idx<=361):\n",
    "            batch_x1,batch_x2,batch_x3,batch_x4 = test_w1[idx].to(self.device), \\\n",
    "                                        test_w2[idx].to(self.device),\\\n",
    "                                        test_w3[idx].to(self.device),\\\n",
    "                                        test_w4[idx].to(self.device)\n",
    "            # print(\"Idx: \", {idx}, \" \", {_})\n",
    "            # print(\"19\")\n",
    "            batch_weekly = [batch_x1,batch_x2,batch_x3,batch_x4][-self.no_of_weeks_to_look_back:]\n",
    "            # print(\"20\")\n",
    "            pred_reg, pred_cls = self.forward(batch_weekly)\n",
    "            pred_reg, pred_cls = pred_reg.cpu().detach().numpy(), pred_cls.cpu().detach().numpy()\n",
    "            y_pred_all_reg.extend(pred_reg.tolist())\n",
    "            y_pred_all_cls.extend(pred_cls.tolist())\n",
    "        return y_pred_all_reg, y_pred_all_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c20e6733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MRR(test_y, pred_y, k=5):\n",
    "    predict = pd.DataFrame([])\n",
    "    # print(pred_y.shape)\n",
    "    # print(test_y.shape)\n",
    "    predict[\"pred_y\"] = pred_y\n",
    "    predict[\"y\"] = test_y\n",
    "\n",
    "    predict = predict.sort_values(\"pred_y\", ascending=False).reset_index(drop=True)\n",
    "    predict[\"pred_y_rank_index\"] = (predict.index) + 1\n",
    "    predict = predict.sort_values(\"y\", ascending=False)\n",
    "\n",
    "    return sum(1 / predict[\"pred_y_rank_index\"][:k])\n",
    "\n",
    "\n",
    "def Precision(test_y, pred_y, k=5):\n",
    "    # print(pred_y.shape)\n",
    "    # print(test_y.shape)\n",
    "    predict = pd.DataFrame([])\n",
    "    predict[\"pred_y\"] = pred_y\n",
    "    predict[\"y\"] = test_y\n",
    "\n",
    "    predict1 = predict.sort_values(\"pred_y\", ascending=False)\n",
    "    predict2 = predict.sort_values(\"y\", ascending=False)\n",
    "    correct = len(list(set(predict1[\"y\"][:k].index) & set(predict2[\"y\"][:k].index)))\n",
    "    return correct / k\n",
    "\n",
    "\n",
    "def IRR(test_y, pred_y, k=5):\n",
    "    # print(pred_y.shape)\n",
    "    # print(test_y.shape)\n",
    "    predict = pd.DataFrame([])\n",
    "    predict[\"pred_y\"] = pred_y\n",
    "    predict[\"y\"] = test_y\n",
    "\n",
    "    predict1 = predict.sort_values(\"pred_y\", ascending=False)\n",
    "    predict2 = predict.sort_values(\"y\", ascending=False)\n",
    "    return sum(predict2[\"y\"][:k]) - sum(predict1[\"y\"][:k])\n",
    "\n",
    "\n",
    "def Acc(test_y, pred_y):\n",
    "    test_y = np.ravel(test_y)\n",
    "    pred_y = np.ravel(pred_y)\n",
    "    pred_y = (pred_y > 0) * 1\n",
    "    pred_y = pred_y[:18050]\n",
    "    # print((test_y.shape))\n",
    "    # print((pred_y.shape))\n",
    "    sum = 0\n",
    "    for i in range(len(pred_y)):\n",
    "        if(test_y[i]==pred_y[i]):\n",
    "            sum += 1\n",
    "    acc_score = sum / len(pred_y)\n",
    "\n",
    "    return acc_score\n",
    "\n",
    "def top_stocks(pred, top_k=5):\n",
    "    NIFTY50_name = pd.read_csv(\"D:\\\\capstone_project\\\\capstone_project\\\\NIFTY50_category.csv\")\n",
    "    with_indices = [[pred[i], i - 18000] for i in range(18000, 18050)]\n",
    "    \n",
    "    # Sort with_indices by the first element of each sublist in descending order\n",
    "    with_indices.sort(key=lambda x: x[0])\n",
    "    \n",
    "    print(\"The top\", top_k, \"Stocks are:\")\n",
    "    print(\"-----------------------------------\")\n",
    "    for i in range(top_k):\n",
    "        print(NIFTY50_name.iloc[with_indices[i][1]]['company'], \n",
    "              \", Sector:\", NIFTY50_name.iloc[with_indices[i][1]]['category'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63699818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    global predictions\n",
    "    global test_y\n",
    "    # global predictions\n",
    "    l2 = 0\n",
    "    lr = 0.05\n",
    "    beta = 1\n",
    "    gamma = 1\n",
    "    alpha = 1\n",
    "    device = \"cpu\"\n",
    "    epochs = 5\n",
    "    hidden_dim = 16\n",
    "    use_gru = False\n",
    "    model = CategoricalGraphAtt(input_dim, time_step, hidden_dim, inner_edge, outer_edge, agg_week_num, use_gru, device).to(device)\n",
    "\n",
    "    # if model_name == \"CG\":\n",
    "    #     pass\n",
    "    #     # model = CategoricalGraph(input_dim, time_step, hidden_dim, inner10_edge, outer_edge, agg_week_num, device).to(device)\n",
    "    # elif model_name == \"CAT\":\n",
    "    #     model = CategoricalGraphAtt(input_dim, time_step, hidden_dim, inner_edge, outer_edge, agg_week_num, use_gru, device).to(device)\n",
    "    # elif model_name == \"CPool\":\n",
    "    #     pass\n",
    "    #     # model = CategoricalGraphPool(input_dim, time_step, hidden_dim, inner_edge, inner20_edge, outer_edge, agg_week_num, use_gru, device).to(device)\n",
    "\n",
    "    # Initialize parameters\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"Number of parameters:%s\" % pytorch_total_params)\n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "    # Optimizer & loss\n",
    "    optimizer = optim.Adam(model.parameters(), weight_decay=l2, lr=lr)\n",
    "    reg_loss_func = nn.MSELoss(reduction='none')\n",
    "    cls_loss_func = nn.BCELoss(reduction=\"none\")\n",
    "\n",
    "    # Save best model\n",
    "    best_metric_IRR = None\n",
    "    best_metric_MRR = None\n",
    "    best_results_IRR = None\n",
    "    best_results_MRR = None\n",
    "    global_best_IRR = 999\n",
    "    global_best_MRR = 0\n",
    "\n",
    "    r_loss = torch.tensor([]).float().to(device)\n",
    "    c_loss = torch.tensor([]).float().to(device)\n",
    "    ra_loss = torch.tensor([]).float().to(device)\n",
    "    for epoch in range(epochs):\n",
    "        for week in range(num_weeks):\n",
    "            # print(\"Epoch: \", {epoch}, \" Week: \", {week})\n",
    "            model.train()  # Prep to train model\n",
    "            batch_x1, batch_x2, batch_x3, batch_x4 = (\n",
    "                train_w1[week].to(device),\n",
    "                train_w2[week].to(device),\n",
    "                train_w3[week].to(device),\n",
    "                train_w4[week].to(device),\n",
    "            )\n",
    "            batch_weekly = [batch_x1, batch_x2, batch_x3, batch_x4][-agg_week_num:]\n",
    "            batch_reg_y = train_reg[week].view(-1, 1).to(device)\n",
    "            batch_cls_y = train_cls[week].view(-1, 1).to(device)\n",
    "            reg_out, cls_out = model(batch_weekly)\n",
    "            reg_out, cls_out = reg_out.view(-1, 1), cls_out.view(-1, 1)\n",
    "\n",
    "            # Calculate loss\n",
    "            reg_loss = reg_loss_func(reg_out, batch_reg_y)  # (target_size, 1)\n",
    "            cls_loss = cls_loss_func(cls_out, batch_cls_y)\n",
    "            rank_loss = torch.relu(-(reg_out.view(-1, 1) * reg_out.view(1, -1)) * (batch_reg_y.view(-1, 1) * batch_reg_y.view(1, -1)))\n",
    "            c_loss = torch.cat((c_loss, cls_loss.view(-1, 1)))\n",
    "            r_loss = torch.cat((r_loss, reg_loss.view(-1, 1)))\n",
    "            ra_loss = torch.cat((ra_loss, rank_loss.view(-1, 1)))\n",
    "\n",
    "            if (week + 1) % 1 == 0:\n",
    "                cls_loss = beta * torch.mean(c_loss)\n",
    "                reg_loss = alpha * torch.mean(r_loss)\n",
    "                rank_loss = gamma * torch.sum(ra_loss)\n",
    "                loss = reg_loss\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                r_loss = torch.tensor([]).float().to(device)\n",
    "                c_loss = torch.tensor([]).float().to(device)\n",
    "                ra_loss = torch.tensor([]).float().to(device)\n",
    "                if (week + 1) % 144 == 0:\n",
    "                    print(\"REG Loss:%.4f CLS Loss:%.4f RANK Loss:%.4f  Loss:%.4f\" % (reg_loss.item(), cls_loss.item(), rank_loss.item(), loss.item()))\n",
    "\n",
    "        \n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        print(\"Evaluate at epoch %s\" % (epoch + 1))\n",
    "        print(\"-----------------------------------------------\")\n",
    "        y_pred, y_pred_cls = model.predict_toprank([test_w1, test_w2, test_w3, test_w4], device, top_k=5)\n",
    "\n",
    "        # Calculate metric\n",
    "        y_pred = np.array(y_pred).ravel()\n",
    "        test_y = np.array(test_y).ravel()\n",
    "        mae = round(mean_absolute_error(test_y, y_pred[:18050]), 4)\n",
    "        acc_score = Acc(test_cls, y_pred)\n",
    "        print(\"Accuracy: \", acc_score)\n",
    "        print(\"-----------------------------------------------\")\n",
    "        predictions = []\n",
    "        y_pred = y_pred[:18050]\n",
    "        predictions = y_pred\n",
    "        # print(y_pred.shape)\n",
    "        results = []\n",
    "        for k in ks_list:\n",
    "            IRRs, MRRs, Prs = [], [], []\n",
    "            for i in range(test_shape):\n",
    "                # print(\"Step: \", {i})\n",
    "                M = MRR(np.array(test_y[loop_number * i : loop_number * (i + 1)]), np.array(y_pred[loop_number * i : loop_number * (i + 1)]), k=k)\n",
    "                MRRs.append(M)\n",
    "                P = Precision(\n",
    "                    np.array(test_y[loop_number * i : loop_number * (i + 1)]), np.array(y_pred[loop_number * i : loop_number * (i + 1)]), k=k\n",
    "                )\n",
    "                Prs.append(P)\n",
    "            over_all = [mae, round(acc_score, 4), round(np.mean(MRRs), 4), round(np.mean(Prs), 4)]\n",
    "            results.append(over_all)\n",
    "        \n",
    "        # print(\"hi\")\n",
    "        # print(results)\n",
    "\n",
    "        performance = [round(mae, 4), round(acc_score, 4), round(np.mean(MRRs), 4), round(np.mean(Prs), 4)]\n",
    "\n",
    "        if np.mean(MRRs) > global_best_MRR:\n",
    "            global_best_MRR = np.mean(MRRs)\n",
    "            best_metric_MRR = performance\n",
    "            best_results_MRR =  results\n",
    "    \n",
    "\n",
    "\n",
    "        # print(\"Best MRR metric: %s\" % best_metric_MRR)\n",
    "        # print(\"Best IRR metric: %s\" % best_metric_IRR)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Finished Training!\")\n",
    "    print(\"-----------------------------------------------\")\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), os.path.join(\"D:\\\\capstone_project\\\\capstone_project\", \"CAT\" + \"_model.pth\"))\n",
    "    top_stocks(predictions)\n",
    "    print(\"-----------------------------------------------\")\n",
    "    return best_metric_IRR, best_metric_MRR, best_results_IRR, best_results_MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6816a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:7620\n",
      "-----------------------------------------------\n",
      "Evaluate at epoch 1\n",
      "-----------------------------------------------\n",
      "Accuracy:  0.5234903047091413\n",
      "-----------------------------------------------\n",
      "Evaluate at epoch 2\n",
      "-----------------------------------------------\n",
      "Accuracy:  0.5234903047091413\n",
      "-----------------------------------------------\n",
      "Evaluate at epoch 3\n",
      "-----------------------------------------------\n",
      "Accuracy:  0.47650969529085874\n",
      "-----------------------------------------------\n",
      "Evaluate at epoch 4\n",
      "-----------------------------------------------\n",
      "Accuracy:  0.47650969529085874\n",
      "-----------------------------------------------\n",
      "Evaluate at epoch 5\n",
      "-----------------------------------------------\n",
      "Accuracy:  0.47650969529085874\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_metric_IRR, best_metric_MRR, best_results_IRR, best_results_MRR = train()\n",
    "print()\n",
    "print(\"-------Final result-------\")\n",
    "print(\"[BEST MRR] MAE:%.4f ACC:%.4f MRR:%.4f Precision:%.4f\" % tuple(best_metric_MRR))\n",
    "for idx, k in enumerate(ks_list):\n",
    "    print(\"[BEST RESULT MRR with k=%s] MAE:%.4f ACC:%.4f MRR:%.4f Precision:%.4f\" % tuple(tuple([k])+tuple(best_results_MRR[idx])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hands_on_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
